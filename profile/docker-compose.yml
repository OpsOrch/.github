# OpsOrch Complete Stack
# This Docker Compose file sets up a complete OpsOrch environment with:
# - OpsOrch Mock Adapters (includes Core + all mock providers with demo data)
# - OpsOrch MCP server (AI tools layer)
# - OpsOrch Console OSS (web UI)
#
# Usage:
#   curl -O https://raw.githubusercontent.com/OpsOrch/.github/main/profile/docker-compose.yml
#   docker compose up -d
#   
# Access:
#   - Console UI: http://localhost:3000
#   - Core API: http://localhost:8080
#   - MCP Server: http://localhost:7070/mcp
#
# Health checks:
#   curl http://localhost:8080/health
#   curl http://localhost:7070/mcp -H 'Content-Type: application/json' -d '{"jsonrpc":"2.0","id":1,"method":"tools/list"}'

version: '3.8'

services:
  # OpsOrch Mock Adapters (includes Core + all mock providers with demo data)
  opsorch-mock-adapters:
    image: ghcr.io/opsorch/opsorch-mock-adapters:latest
    container_name: opsorch-mock-adapters
    ports:
      - "8080:8080"
    environment:
      # Core configuration
      - OPSORCH_BEARER_TOKEN=demo
      - OPSORCH_LOG_LEVEL=info
      - OPSORCH_PORT=8080
      
      # Mock providers are pre-configured and provide demo data for:
      # - Incidents, Logs, Metrics, Tickets, Services, Messaging, Secrets
    # Health check removed - container has no curl/wget available
    # Service is verified to be working via manual endpoint tests
    restart: unless-stopped
    networks:
      - opsorch

  # OpsOrch MCP Server (AI Tools Layer)
  opsorch-mcp:
    image: ghcr.io/opsorch/opsorch-mcp:latest
    container_name: opsorch-mcp
    ports:
      - "7070:7070"
    environment:
      # Connect to Mock Adapters service
      - OPSORCH_CORE_URL=http://opsorch-mock-adapters:8080
      - OPSORCH_CORE_TOKEN=demo
      - OPSORCH_CORE_TIMEOUT_MS=15000
      
      # MCP server configuration
      - MCP_HTTP_PORT=7070
      - OPSORCH_LOG_LEVEL=info
      
      # CORS configuration for web access
      - MCP_HTTP_ALLOW_ORIGINS=http://localhost:3000,http://opsorch-console:3000
      - MCP_HTTP_ALLOW_HOSTS=localhost,opsorch-console
    depends_on:
      - opsorch-mock-adapters
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7070/mcp", "-H", "Content-Type: application/json", "-H", "Accept: application/json, text/event-stream", "-d", "{\"jsonrpc\":\"2.0\",\"id\":1,\"method\":\"tools/list\"}"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
    restart: unless-stopped
    networks:
      - opsorch

  # OpsOrch Console OSS (Web UI)
  opsorch-console:
    image: ghcr.io/opsorch/opsorch-console:latest-oss
    container_name: opsorch-console
    ports:
      - "3000:3000"
    environment:
      # Server-side API URL (used by Next.js API proxy routes)
      - OPSORCH_API_BASE_URL=http://opsorch-mock-adapters:8080
      
      # Client-side API URL (used by browser)
      - NEXT_PUBLIC_OPSORCH_CORE_URL=http://localhost:8080
      
      # Edition configuration
      - NEXT_PUBLIC_OPSORCH_EDITION=oss
      
      # Optional: Connect to Copilot (if available)
      # - NEXT_PUBLIC_COPILOT_URL=http://localhost:6060
      
      # Optional: Configure Copilot LLM Provider
      # - LLM_PROVIDER=gemini  # or "openai", "anthropic"
      # - GEMINI_API_KEY=your-api-key
      # - GEMINI_MODEL=gemini-3-flash-preview  # optional, latest model
    depends_on:
      - opsorch-mock-adapters
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    restart: unless-stopped
    networks:
      - opsorch

networks:
  opsorch:
    driver: bridge
    name: opsorch-network

# Optional: Add volumes for persistent data
# volumes:
#   opsorch-data:
#     driver: local
